{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "##Inspired from [@ggerganov](https://github.com/ggerganov/whisper.cpp/blob/6f82320b053bb8183a1734e09c940d6bf2a0f4b2/models/convert-pt-to-ggml.py) "
   ],
   "metadata": {
    "id": "qPWqt6hU9AbT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Clone whisper repository"
   ],
   "metadata": {
    "id": "J8_qz3GD9S3T"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kzziH8swyL6Z",
    "outputId": "e561fbda-f5e3-4ddb-c56b-565b3efc56a1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "fatal: destination path 'whisper' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Download tiny openai/whisper model "
   ],
   "metadata": {
    "id": "bA2_mSFP9Wo7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt\n",
    "!wget https://openaipublic.azureedge.net/main/whisper/models/d3dd57d32accea0b295c96e26691aa14d8822fac7d9d27d5dc00b4ca2826dd03/tiny.en.pt"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x60j4DTT2zs9",
    "outputId": "d3553fe1-12db-4966-8421-ecb0cc6610cf"
   },
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2022-10-28 20:57:23--  https://openaipublic.azureedge.net/main/whisper/models/65147644a518d12f04e32d6f3b26facc3f8dd46e5390956a9424a650c0ce22b9/tiny.pt\n",
      "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.18, 2620:1ec:bdf::70, 2620:1ec:46::70\n",
      "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75572083 (72M) [application/octet-stream]\n",
      "Saving to: ‘tiny.pt.1’\n",
      "\n",
      "tiny.pt.1           100%[===================>]  72.07M  60.5MB/s    in 1.2s    \n",
      "\n",
      "2022-10-28 20:57:24 (60.5 MB/s) - ‘tiny.pt.1’ saved [75572083/75572083]\n",
      "\n",
      "--2022-10-28 20:57:24--  https://openaipublic.azureedge.net/main/whisper/models/d3dd57d32accea0b295c96e26691aa14d8822fac7d9d27d5dc00b4ca2826dd03/tiny.en.pt\n",
      "Resolving openaipublic.azureedge.net (openaipublic.azureedge.net)... 13.107.246.18, 2620:1ec:bdf::70\n",
      "Connecting to openaipublic.azureedge.net (openaipublic.azureedge.net)|13.107.246.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 75571315 (72M) [application/octet-stream]\n",
      "Saving to: ‘tiny.en.pt.1’\n",
      "\n",
      "tiny.en.pt.1        100%[===================>]  72.07M  38.8MB/s    in 1.9s    \n",
      "\n",
      "2022-10-28 20:57:26 (38.8 MB/s) - ‘tiny.en.pt.1’ saved [75571315/75571315]\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Build tokenzer"
   ],
   "metadata": {
    "id": "yJPzCkef9fX_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install transformers\n",
    "from transformers import GPTJForCausalLM\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "# ref: https://github.com/openai/whisper/blob/8cf36f3508c9acd341a45eb2364239a3d81458b9/whisper/tokenizer.py#L273-L292\n",
    "def build_tokenizer(path_to_whisper_repo: str, name: str = \"gpt2\"):\n",
    "    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "    path = os.path.join(path_to_whisper_repo, \"whisper/assets\", name)\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(path)\n",
    "\n",
    "    specials = [\n",
    "        \"<|startoftranscript|>\",\n",
    "        *[f\"<|{lang}|>\" for lang in LANGUAGES.keys()],\n",
    "        \"<|translate|>\",\n",
    "        \"<|transcribe|>\",\n",
    "        \"<|startoflm|>\",\n",
    "        \"<|startofprev|>\",\n",
    "        \"<|nocaptions|>\",\n",
    "        \"<|notimestamps|>\",\n",
    "    ]\n",
    "\n",
    "    tokenizer.add_special_tokens(dict(additional_special_tokens=specials))\n",
    "    return tokenizer"
   ],
   "metadata": {
    "id": "s5XoZR774zDy",
    "outputId": "d742c5c3-e2f2-45f4-adab-d75f9b4f6e7d",
    "colab": {
     "base_uri": "https://localhost:8080/"
    }
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.9.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# ref: https://github.com/openai/whisper/blob/8cf36f3508c9acd341a45eb2364239a3d81458b9/whisper/tokenizer.py#L10-L110\n",
    "LANGUAGES = {\n",
    "    \"en\": \"english\",\n",
    "    \"zh\": \"chinese\",\n",
    "    \"de\": \"german\",\n",
    "    \"es\": \"spanish\",\n",
    "    \"ru\": \"russian\",\n",
    "    \"ko\": \"korean\",\n",
    "    \"fr\": \"french\",\n",
    "    \"ja\": \"japanese\",\n",
    "    \"pt\": \"portuguese\",\n",
    "    \"tr\": \"turkish\",\n",
    "    \"pl\": \"polish\",\n",
    "    \"ca\": \"catalan\",\n",
    "    \"nl\": \"dutch\",\n",
    "    \"ar\": \"arabic\",\n",
    "    \"sv\": \"swedish\",\n",
    "    \"it\": \"italian\",\n",
    "    \"id\": \"indonesian\",\n",
    "    \"hi\": \"hindi\",\n",
    "    \"fi\": \"finnish\",\n",
    "    \"vi\": \"vietnamese\",\n",
    "    \"iw\": \"hebrew\",\n",
    "    \"uk\": \"ukrainian\",\n",
    "    \"el\": \"greek\",\n",
    "    \"ms\": \"malay\",\n",
    "    \"cs\": \"czech\",\n",
    "    \"ro\": \"romanian\",\n",
    "    \"da\": \"danish\",\n",
    "    \"hu\": \"hungarian\",\n",
    "    \"ta\": \"tamil\",\n",
    "    \"no\": \"norwegian\",\n",
    "    \"th\": \"thai\",\n",
    "    \"ur\": \"urdu\",\n",
    "    \"hr\": \"croatian\",\n",
    "    \"bg\": \"bulgarian\",\n",
    "    \"lt\": \"lithuanian\",\n",
    "    \"la\": \"latin\",\n",
    "    \"mi\": \"maori\",\n",
    "    \"ml\": \"malayalam\",\n",
    "    \"cy\": \"welsh\",\n",
    "    \"sk\": \"slovak\",\n",
    "    \"te\": \"telugu\",\n",
    "    \"fa\": \"persian\",\n",
    "    \"lv\": \"latvian\",\n",
    "    \"bn\": \"bengali\",\n",
    "    \"sr\": \"serbian\",\n",
    "    \"az\": \"azerbaijani\",\n",
    "    \"sl\": \"slovenian\",\n",
    "    \"kn\": \"kannada\",\n",
    "    \"et\": \"estonian\",\n",
    "    \"mk\": \"macedonian\",\n",
    "    \"br\": \"breton\",\n",
    "    \"eu\": \"basque\",\n",
    "    \"is\": \"icelandic\",\n",
    "    \"hy\": \"armenian\",\n",
    "    \"ne\": \"nepali\",\n",
    "    \"mn\": \"mongolian\",\n",
    "    \"bs\": \"bosnian\",\n",
    "    \"kk\": \"kazakh\",\n",
    "    \"sq\": \"albanian\",\n",
    "    \"sw\": \"swahili\",\n",
    "    \"gl\": \"galician\",\n",
    "    \"mr\": \"marathi\",\n",
    "    \"pa\": \"punjabi\",\n",
    "    \"si\": \"sinhala\",\n",
    "    \"km\": \"khmer\",\n",
    "    \"sn\": \"shona\",\n",
    "    \"yo\": \"yoruba\",\n",
    "    \"so\": \"somali\",\n",
    "    \"af\": \"afrikaans\",\n",
    "    \"oc\": \"occitan\",\n",
    "    \"ka\": \"georgian\",\n",
    "    \"be\": \"belarusian\",\n",
    "    \"tg\": \"tajik\",\n",
    "    \"sd\": \"sindhi\",\n",
    "    \"gu\": \"gujarati\",\n",
    "    \"am\": \"amharic\",\n",
    "    \"yi\": \"yiddish\",\n",
    "    \"lo\": \"lao\",\n",
    "    \"uz\": \"uzbek\",\n",
    "    \"fo\": \"faroese\",\n",
    "    \"ht\": \"haitian creole\",\n",
    "    \"ps\": \"pashto\",\n",
    "    \"tk\": \"turkmen\",\n",
    "    \"nn\": \"nynorsk\",\n",
    "    \"mt\": \"maltese\",\n",
    "    \"sa\": \"sanskrit\",\n",
    "    \"lb\": \"luxembourgish\",\n",
    "    \"my\": \"myanmar\",\n",
    "    \"bo\": \"tibetan\",\n",
    "    \"tl\": \"tagalog\",\n",
    "    \"mg\": \"malagasy\",\n",
    "    \"as\": \"assamese\",\n",
    "    \"tt\": \"tatar\",\n",
    "    \"haw\": \"hawaiian\",\n",
    "    \"ln\": \"lingala\",\n",
    "    \"ha\": \"hausa\",\n",
    "    \"ba\": \"bashkir\",\n",
    "    \"jw\": \"javanese\",\n",
    "    \"su\": \"sundanese\",\n",
    "}"
   ],
   "metadata": {
    "id": "LJU0X5QJ5TWa"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# ref: https://github.com/openai/gpt-2/blob/master/src/encoder.py\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))"
   ],
   "metadata": {
    "id": "ubnuRn2475bg"
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Generate filters and vocab binary"
   ],
   "metadata": {
    "id": "rqPLYlya9lE9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import struct\n",
    "import json\n",
    "import code\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "fname_inp = \"/content/tiny.en.pt\"\n",
    "# try to load PyTorch binary data\n",
    "try:\n",
    "    model_bytes = open(fname_inp, \"rb\").read()\n",
    "    with io.BytesIO(model_bytes) as fp:\n",
    "        checkpoint = torch.load(fp, map_location=\"cpu\")\n",
    "except:\n",
    "    print(\"Error: failed to load PyTorch model file: %s\" % fname_inp)\n",
    "    sys.exit(1)\n",
    "\n",
    "whisperparams = checkpoint[\"dims\"]\n",
    "print(\"whisperparams:\", whisperparams)\n",
    "list_vars = checkpoint[\"model_state_dict\"]\n",
    "#print(list_vars['encoder.positional_embedding'])\n",
    "#print(list_vars['encoder.conv1.weight'])\n",
    "#print(list_vars['encoder.conv1.weight'].shape)\n",
    "\n",
    "# load mel filters\n",
    "n_mels = whisperparams[\"n_mels\"]\n",
    "with np.load(os.path.join(\"/content/whisper/whisper/assets\", \"mel_filters.npz\")) as f:\n",
    "    filters = torch.from_numpy(f[f\"mel_{n_mels}\"])\n",
    "    #print (filters)\n",
    "\n",
    "multilingual = whisperparams[\"n_vocab\"] == 51865\n",
    "tokenizer = build_tokenizer(\"/content/whisper/\", multilingual and \"multilingual\" or \"gpt2\")\n",
    "\n",
    "#print(tokenizer)\n",
    "#print(tokenizer.name_or_path)\n",
    "#print(len(tokenizer.additional_special_tokens))\n",
    "dir_tokenizer = tokenizer.name_or_path\n",
    "print(\"dir_tokenizer:\",dir_tokenizer)\n",
    "\n",
    "# output in the same directory as the model\n",
    "fname_out = \"./filters_vocab_gen.bin\"\n",
    "with open(dir_tokenizer + \"/vocab.json\", \"r\", encoding=\"utf8\") as f:\n",
    "    tokens = json.load(f)\n",
    "fout = open(fname_out, \"wb\")\n",
    "fout.write(struct.pack(\"i\", 0x5553454e)) # magic: USEN in hex    \n",
    "#fout.write(struct.pack(\"i\", whisperparams[\"n_vocab\"]))\n",
    "#print(\"n_vocab:\",whisperparams[\"n_vocab\"])\n",
    "#fout.write(struct.pack(\"i\", whisperparams[\"n_mels\"]))\n",
    "#print(\"n_mels:\",n_mels)\n",
    "# write mel filters\n",
    "fout.write(struct.pack(\"i\", filters.shape[0]))\n",
    "print(\"filters.shape[0]:\",filters.shape[0])\n",
    "fout.write(struct.pack(\"i\", filters.shape[1]))\n",
    "print(\"filters.shape[0]:\",filters.shape[1])\n",
    "for i in range(filters.shape[0]):\n",
    "    for j in range(filters.shape[1]):\n",
    "        fout.write(struct.pack(\"f\", filters[i][j]))\n",
    "byte_encoder = bytes_to_unicode()\n",
    "byte_decoder = {v:k for k, v in byte_encoder.items()}\n",
    "\n",
    "fout.write(struct.pack(\"i\", len(tokens)))\n",
    "print(\"len(tokens):\",len(tokens))\n",
    "for key in tokens:\n",
    "    text = bytearray([byte_decoder[c] for c in key])\n",
    "    fout.write(struct.pack(\"i\", len(text)))\n",
    "    fout.write(text)\n",
    "fout.close()\n",
    "\n",
    "print(\"Done. Output file: \" + fname_out)\n",
    "print(\"\")    \n",
    "        "
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6xl0Bn30J08",
    "outputId": "1b5d5b3a-394e-4dd1-f9ed-cd082ab9ddca"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "whisperparams: {'n_mels': 80, 'n_vocab': 51864, 'n_audio_ctx': 1500, 'n_audio_state': 384, 'n_audio_head': 6, 'n_audio_layer': 4, 'n_text_ctx': 448, 'n_text_state': 384, 'n_text_head': 6, 'n_text_layer': 4}\n",
      "dir_tokenizer: /content/whisper/whisper/assets/gpt2\n",
      "filters.shape[0]: 80\n",
      "filters.shape[0]: 201\n",
      "len(tokens): 50257\n",
      "Done. Output file: ./filters_vocab_gen.bin\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "z0fUzw8M8838"
   }
  }
 ]
}